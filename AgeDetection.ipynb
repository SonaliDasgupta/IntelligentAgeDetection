{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip test_Bh8pGW3.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip train_DETg9GD.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip IMFDB_final.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head IMFDB_final/AamairKhan/3Idiots/3Idiots.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>377.jpg</td>\n",
       "      <td>MIDDLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17814.jpg</td>\n",
       "      <td>YOUNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21283.jpg</td>\n",
       "      <td>MIDDLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16496.jpg</td>\n",
       "      <td>YOUNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4487.jpg</td>\n",
       "      <td>MIDDLE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID   Class\n",
       "0    377.jpg  MIDDLE\n",
       "1  17814.jpg   YOUNG\n",
       "2  21283.jpg  MIDDLE\n",
       "3  16496.jpg   YOUNG\n",
       "4   4487.jpg  MIDDLE"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MIDDLE', 'YOUNG', 'OLD'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19906, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import backend as k \n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "\n",
    "\n",
    "img_width, img_height = 256, 256\n",
    "train_data_dir = \"Train\"\n",
    "validation_data_dir = \"Train\"\n",
    "nb_train_samples = 0.8*len(train_df)\n",
    "nb_validation_samples = 0.2*len(train_df) \n",
    "batch_size = 16\n",
    "epochs = 50\n",
    "\n",
    "def normalCNN():\n",
    "    \n",
    "    model = applications.ResNet50(include_top=False, weights='imagenet', input_shape=(img_width, img_height, 3))\n",
    "\n",
    "\n",
    "\n",
    "    # Freeze the layers which you don't want to train. Here I am freezing the all layers.\n",
    "    for layer in model.layers[:]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Adding custom Layer\n",
    "    # We only add\n",
    "    x = model.output\n",
    "    x = Flatten()(x)\n",
    "    # Adding even more custom layers\n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "    predictions = Dense(3, activation=\"softmax\")(x)\n",
    "\n",
    "    # creating the final model \n",
    "    model_final = Model(input = model.input, output = predictions)\n",
    "    return model_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom logic to read labels in-memory from the csv file, and omages as a sequence from image folder\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.utils import Sequence\n",
    "import math\n",
    "import random\n",
    "\n",
    "train_datapath = 'Train/'\n",
    "\n",
    "def load_image(im):\n",
    "    img_array = img_to_array(load_img(im, grayscale=True)) / 255.\n",
    "    return np.reshape(img_array, (img_width, img_height, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class ImgDataSequence(Sequence):\\n    \"\"\"\\n    Keras Sequence object to train a model on larger-than-memory data.\\n    \"\"\"\\n    def __init__(self, df, data_path, batch_size, mode=\\'train\\'):\\n        self.df = df\\n        self.bsz = batch_size\\n        self.mode = mode\\n\\n        # Take labels and a list of image locations in memory\\n        self.labels = self.df[\\'Class\\'].values\\n        self.im_list = self.df[\\'ID\\'].apply(lambda x: os.path.join(data_path, x)).tolist()\\n        \\n    def __len__(self):\\n        return int(math.ceil(len(self.df) / float(self.bsz)))\\n\\n    def on_epoch_end(self):\\n        # Shuffles indexes after each epoch if in training mode\\n        self.indexes = range(len(self.im_list))\\n        if self.mode == \\'train\\':\\n            self.indexes = random.sample(self.indexes, k=len(self.indexes))\\n\\n    def get_batch_labels(self, idx):\\n        # Fetch a batch of labels\\n        return self.labels[idx * self.bsz: (idx + 1) * self.bsz]\\n    \\n    def get_batch_features(self, idx):\\n        # Fetch a batch of inputs\\n        return np.array([load_image(im) for im in self.im_list[idx * self.bsz: (1 + idx) * self.bsz]])\\n\\n    def __getitem__(self, idx):\\n        batch_x = self.get_batch_features(idx)\\n        batch_y = self.get_batch_labels(idx)\\n        return batch_x, batch_y'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''class ImgDataSequence(Sequence):\n",
    "    \"\"\"\n",
    "    Keras Sequence object to train a model on larger-than-memory data.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, data_path, batch_size, mode='train'):\n",
    "        self.df = df\n",
    "        self.bsz = batch_size\n",
    "        self.mode = mode\n",
    "\n",
    "        # Take labels and a list of image locations in memory\n",
    "        self.labels = self.df['Class'].values\n",
    "        self.im_list = self.df['ID'].apply(lambda x: os.path.join(data_path, x)).tolist()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(math.ceil(len(self.df) / float(self.bsz)))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Shuffles indexes after each epoch if in training mode\n",
    "        self.indexes = range(len(self.im_list))\n",
    "        if self.mode == 'train':\n",
    "            self.indexes = random.sample(self.indexes, k=len(self.indexes))\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return self.labels[idx * self.bsz: (idx + 1) * self.bsz]\n",
    "    \n",
    "    def get_batch_features(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return np.array([load_image(im) for im in self.im_list[idx * self.bsz: (1 + idx) * self.bsz]])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.get_batch_features(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "        return batch_x, batch_y'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14930 images belonging to 3 classes.\n",
      "Found 4976 images belonging to 3 classes.\n",
      "Found 6636 images.\n"
     ]
    }
   ],
   "source": [
    "# Initiate the train and test generators with data Augumentation \n",
    "\n",
    "datagen=ImageDataGenerator(rescale=1./255.,validation_split=0.25)\n",
    "\n",
    "train_generator=datagen.flow_from_dataframe(dataframe=train_df, directory=\"Train/\", x_col=\"ID\", y_col=\"Class\", subset=\"training\",\n",
    "batch_size=32, seed=42, shuffle=True, class_mode=\"categorical\", target_size=(img_width, img_height))\n",
    "\n",
    "valid_generator=datagen.flow_from_dataframe(dataframe=train_df, directory=\"Train/\", x_col=\"ID\", y_col=\"Class\", subset=\"validation\",\n",
    "batch_size=32, seed=42, shuffle=True, class_mode=\"categorical\", target_size=(img_width, img_height))\n",
    "\n",
    "test_datagen=ImageDataGenerator(rescale=1./255.)\n",
    "\n",
    "test_generator=test_datagen.flow_from_dataframe(dataframe=test_df, directory=\"Test/\", x_col=\"ID\", y_col=None, batch_size=32,\n",
    "seed=42, shuffle=False, class_mode=None, target_size=(img_width, img_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:40: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:21: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., callbacks=[<keras.ca..., epochs=5, validation_data=<keras_pre..., steps_per_epoch=497.0, validation_steps=3981.20000...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "496/497 [============================>.] - ETA: 4s - loss: 7.3520 - acc: 0.5430"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# compile the model\n",
    "model_standard_cnn = normalCNN()\n",
    "model_standard_cnn.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.Adam(), metrics=[\"accuracy\"])\n",
    "\n",
    "# Save the model according to the conditions  \n",
    "checkpoint = ModelCheckpoint(\"resnet50_retrain.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "\n",
    "# Train the model \n",
    "model_standard_cnn.fit_generator(\n",
    "  train_generator,\n",
    "  samples_per_epoch = nb_train_samples,\n",
    "  epochs = 5, #FOR SPEED KEEPING IT TO 5 FOR NOW\n",
    "  validation_data = valid_generator,\n",
    "  nb_val_samples = nb_validation_samples,\n",
    "  callbacks = [checkpoint, early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
